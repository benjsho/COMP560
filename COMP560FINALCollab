{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPxX4W51BivqvrfPl6pnyxK"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"a92e8bea062f40118242e0915f67558f":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_e26f89da8dcc451385037f3bd664f380","IPY_MODEL_223c9f46e1ac426e8a10ff68690b73bd","IPY_MODEL_cdbfe8cc0d564e939b277e1e46140640"],"layout":"IPY_MODEL_afeae54ba892409eb532e1b30d81759c"}},"e26f89da8dcc451385037f3bd664f380":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_18e403c2d3fd4642b1cbfe17bdd9963b","placeholder":"​","style":"IPY_MODEL_72a4f9f5d05e42c5a1cebdb90473958d","value":"Map (num_proc=4): 100%"}},"223c9f46e1ac426e8a10ff68690b73bd":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_52e48b7ef9a147c4a660e56aad5308a8","max":8436,"min":0,"orientation":"horizontal","style":"IPY_MODEL_94f06f73bd7d4f11bbe3ad9c22910daf","value":8436}},"cdbfe8cc0d564e939b277e1e46140640":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_fb8e6c6f3d1640b19b53c84ec0b02a20","placeholder":"​","style":"IPY_MODEL_c7cb7e3106824396a14a757020fc221e","value":" 8436/8436 [00:08&lt;00:00, 2278.66 examples/s]"}},"afeae54ba892409eb532e1b30d81759c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"18e403c2d3fd4642b1cbfe17bdd9963b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"72a4f9f5d05e42c5a1cebdb90473958d":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"52e48b7ef9a147c4a660e56aad5308a8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"94f06f73bd7d4f11bbe3ad9c22910daf":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"fb8e6c6f3d1640b19b53c84ec0b02a20":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c7cb7e3106824396a14a757020fc221e":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"87687cffba6743cabdd1b0820efb3cd1":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_ab9133cf35c84eab93c92b4d98417588","IPY_MODEL_bcae3dd2a45040d193545bef33275c3f","IPY_MODEL_f13bb79e22f842908103785ac226e92f"],"layout":"IPY_MODEL_075e84bbd44e4655b637977aaa0217c8"}},"ab9133cf35c84eab93c92b4d98417588":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_8888e63277154924a2e99d70b8eb2008","placeholder":"​","style":"IPY_MODEL_32b50e9225f94c3cbe5b8cf996ebf1e5","value":"Generating eval split: "}},"bcae3dd2a45040d193545bef33275c3f":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_0c52485a74c4466888ec5f0cc6617bce","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_2da4c3f6dd244de591e831ab4e33c5af","value":1}},"f13bb79e22f842908103785ac226e92f":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_1bc97e08c46342e181f4309c7501c5bc","placeholder":"​","style":"IPY_MODEL_7fdd1615f7534d8d8f29cdd290c01f7a","value":" 843/0 [00:00&lt;00:00, 15140.98 examples/s]"}},"075e84bbd44e4655b637977aaa0217c8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8888e63277154924a2e99d70b8eb2008":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"32b50e9225f94c3cbe5b8cf996ebf1e5":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"0c52485a74c4466888ec5f0cc6617bce":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"20px"}},"2da4c3f6dd244de591e831ab4e33c5af":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"1bc97e08c46342e181f4309c7501c5bc":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7fdd1615f7534d8d8f29cdd290c01f7a":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"6e741ed233eb481ebaaf85d20347734e":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_83bc4884fdcb42eea6bab4fcefccaf0c","IPY_MODEL_3ecdc59d10724254922dfdf750cb2db3","IPY_MODEL_05e41235d32e4d4f89545bbfbe8cd805"],"layout":"IPY_MODEL_0cc6371b502e4e0794cd7ec000d9c986"}},"83bc4884fdcb42eea6bab4fcefccaf0c":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_3a5b42a9ae024ed5a8c678cc7f92ea38","placeholder":"​","style":"IPY_MODEL_7420bd142e564498959a47a01804fb99","value":"Map (num_proc=4): 100%"}},"3ecdc59d10724254922dfdf750cb2db3":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_60ccea7ae58d47c693b82f70dfafd50d","max":843,"min":0,"orientation":"horizontal","style":"IPY_MODEL_2c455a76b78f4a74b021d028dc75fcdf","value":843}},"05e41235d32e4d4f89545bbfbe8cd805":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_64b3e180d5b243ddba1f375a0d02b983","placeholder":"​","style":"IPY_MODEL_0a5ead617fa34b1d9d2ff357638e3223","value":" 843/843 [00:01&lt;00:00, 1132.23 examples/s]"}},"0cc6371b502e4e0794cd7ec000d9c986":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3a5b42a9ae024ed5a8c678cc7f92ea38":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7420bd142e564498959a47a01804fb99":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"60ccea7ae58d47c693b82f70dfafd50d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2c455a76b78f4a74b021d028dc75fcdf":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"64b3e180d5b243ddba1f375a0d02b983":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0a5ead617fa34b1d9d2ff357638e3223":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"KImmI-oU_uge"},"outputs":[],"source":[]},{"cell_type":"code","source":["# check GPU\n","import torch\n","print(\"GPU available?\", torch.cuda.is_available())\n","\n","# if GPU enabled:\n","!pip install --upgrade transformers datasets peft bitsandbytes accelerate\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"W1YEiXDKDOaA","executionInfo":{"status":"ok","timestamp":1745989444442,"user_tz":240,"elapsed":10807,"user":{"displayName":"Carl Wheezer","userId":"01232549880719749531"}},"outputId":"5e8af777-100f-42da-d9f5-9da11be8c5b3"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["GPU available? False\n","Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.51.3)\n","Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (3.5.1)\n","Requirement already satisfied: peft in /usr/local/lib/python3.11/dist-packages (0.15.2)\n","Requirement already satisfied: bitsandbytes in /usr/local/lib/python3.11/dist-packages (0.45.5)\n","Requirement already satisfied: accelerate in /usr/local/lib/python3.11/dist-packages (1.6.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.30.2)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n","Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n","Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n","Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n","Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n","Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.8)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n","Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n","Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.16)\n","Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2025.3.0)\n","Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.15)\n","Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from peft) (5.9.5)\n","Requirement already satisfied: torch>=1.13.0 in /usr/local/lib/python3.11/dist-packages (from peft) (2.6.0+cu124)\n","Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.6.1)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.3.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.6.0)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.4.3)\n","Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.1)\n","Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.20.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.13.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.4.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.1.31)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (3.4.2)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (3.1.6)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (12.4.127)\n","Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (12.4.127)\n","Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (12.4.127)\n","Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (9.1.0.70)\n","Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (12.4.5.8)\n","Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (11.2.1.3)\n","Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (10.3.5.147)\n","Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (11.6.1.9)\n","Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (12.3.1.170)\n","Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (0.6.2)\n","Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (2.21.5)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (12.4.127)\n","Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (12.4.127)\n","Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (3.2.0)\n","Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (1.13.1)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.13.0->peft) (1.3.0)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.9.0.post0)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.13.0->peft) (3.0.2)\n"]}]},{"cell_type":"code","source":["\n"],"metadata":{"id":"wSEC_0sFV9W2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive', force_remount=True)\n","!ls \"/content/drive/My Drive/tinyllama-lora\"\n","\n","from datasets import load_dataset\n","dataset = load_dataset(\n","    'json',\n","    data_files='/content/drive/MyDrive/WizardDatasetFormatted.jsonl',\n","    split='train'\n",")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JvHeTYhFDPS0","executionInfo":{"status":"ok","timestamp":1745989459179,"user_tz":240,"elapsed":9367,"user":{"displayName":"Carl Wheezer","userId":"01232549880719749531"}},"outputId":"d6684ed4-ddd8-400c-c138-209549ca1d68"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n","adapter_config.json\t   checkpoint-2000  checkpoint-4000  runs\n","adapter_model.safetensors  checkpoint-2500  checkpoint-4218\n","checkpoint-1000\t\t   checkpoint-3000  checkpoint-500\n","checkpoint-1500\t\t   checkpoint-3500  README.md\n"]}]},{"cell_type":"code","source":["from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments, Trainer\n","from peft import get_peft_model, LoraConfig\n","import torch\n","\n","model_name = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n","tokenizer  = AutoTokenizer.from_pretrained(model_name)\n","\n","# IF GPU is enabled:\n","model = AutoModelForCausalLM.from_pretrained(\n","    model_name,\n","    load_in_8bit=True,\n","    device_map=\"auto\"\n",")\n","\n","# only CPU:\n","# model = AutoModelForCausalLM.from_pretrained(\n","#     model_name,\n","#     device_map={\"\": \"cpu\"},\n","#     torch_dtype=torch.float16\n","# )\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":446},"id":"LjHS3nMNDRWM","executionInfo":{"status":"error","timestamp":1745989542273,"user_tz":240,"elapsed":1128,"user":{"displayName":"Carl Wheezer","userId":"01232549880719749531"}},"outputId":"a792e77f-9809-4553-a9bd-7e447a22f14a"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stderr","text":["The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n","WARNING:bitsandbytes.cextension:The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable.\n","CUDA is required but not available for bitsandbytes. Please consider installing the multi-platform enabled version of bitsandbytes, which is currently a work in progress. Please check currently supported platforms and installation instructions at https://huggingface.co/docs/bitsandbytes/main/en/installation#multi-backend\n"]},{"output_type":"error","ename":"RuntimeError","evalue":"CUDA is required but not available for bitsandbytes. Please consider installing the multi-platform enabled version of bitsandbytes, which is currently a work in progress. Please check currently supported platforms and installation instructions at https://huggingface.co/docs/bitsandbytes/main/en/installation#multi-backend","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m<ipython-input-4-8523070f94e6>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# IF GPU is enabled:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m model = AutoModelForCausalLM.from_pretrained(\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0mmodel_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mload_in_8bit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/auto/auto_factory.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    569\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mmodel_class\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig_class\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msub_configs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"text_config\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    570\u001b[0m                 \u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_text_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 571\u001b[0;31m             return model_class.from_pretrained(\n\u001b[0m\u001b[1;32m    572\u001b[0m                 \u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mmodel_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mhub_kwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    573\u001b[0m             )\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36m_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    277\u001b[0m         \u001b[0mold_dtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_default_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    278\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 279\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    280\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    281\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_default_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mold_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   4226\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4227\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhf_quantizer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4228\u001b[0;31m             hf_quantizer.validate_environment(\n\u001b[0m\u001b[1;32m   4229\u001b[0m                 \u001b[0mtorch_dtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch_dtype\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4230\u001b[0m                 \u001b[0mfrom_tf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfrom_tf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/quantizers/quantizer_bnb_8bit.py\u001b[0m in \u001b[0;36mvalidate_environment\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m         \u001b[0mbnb_multibackend_is_enabled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mis_bitsandbytes_multi_backend_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m         \u001b[0mvalidate_bnb_backend_availability\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraise_exception\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"from_tf\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"from_flax\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/integrations/bitsandbytes.py\u001b[0m in \u001b[0;36mvalidate_bnb_backend_availability\u001b[0;34m(raise_exception)\u001b[0m\n\u001b[1;32m    559\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mis_bitsandbytes_multi_backend_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    560\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_validate_bnb_multi_backend_availability\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraise_exception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 561\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_validate_bnb_cuda_backend_availability\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraise_exception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/integrations/bitsandbytes.py\u001b[0m in \u001b[0;36m_validate_bnb_cuda_backend_availability\u001b[0;34m(raise_exception)\u001b[0m\n\u001b[1;32m    537\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mraise_exception\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    538\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlog_msg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 539\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlog_msg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    540\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    541\u001b[0m         \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlog_msg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: CUDA is required but not available for bitsandbytes. Please consider installing the multi-platform enabled version of bitsandbytes, which is currently a work in progress. Please check currently supported platforms and installation instructions at https://huggingface.co/docs/bitsandbytes/main/en/installation#multi-backend"]}]},{"cell_type":"code","source":[],"metadata":{"id":"33uHRRbyIOWE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from transformers import DataCollatorForLanguageModeling\n","\n","# define preprocessing\n","max_in, max_out = 512, 128\n","def preprocess(ex):\n","    # tokenize prompt + completion separately\n","    p = tokenizer(ex[\"prompt\"], truncation=True, max_length=max_in)\n","    c = tokenizer(ex[\"completion\"], truncation=True, max_length=max_out)\n","\n","    # full sequence = prompt_ids + completion_ids\n","    input_ids = p.input_ids + c.input_ids\n","    attention_mask = [1] * len(input_ids)\n","\n","    # labels: ignore prompt tokens\n","    labels = [-100] * len(p.input_ids) + c.input_ids\n","\n","    return {\"input_ids\": input_ids,\n","            \"attention_mask\": attention_mask,\n","            \"labels\": labels}\n","\n","# mapping\n","tok_ds = dataset.map(\n","    preprocess,\n","    remove_columns=[\"prompt\", \"completion\"],\n","    num_proc=4  # you can reduce if Colab is limited\n",")\n","\n","data_collator = DataCollatorForLanguageModeling(\n","    tokenizer=tokenizer,\n","    mlm=False,              # causal LM, not masked LM\n","    # pad_to_multiple_of=8  pad lengths to multiples of 8 for speed\n",")\n","\n"],"metadata":{"id":"NfxzBR6BFefE","executionInfo":{"status":"ok","timestamp":1745989895537,"user_tz":240,"elapsed":8256,"user":{"displayName":"Carl Wheezer","userId":"01232549880719749531"}},"colab":{"base_uri":"https://localhost:8080/","height":49,"referenced_widgets":["a92e8bea062f40118242e0915f67558f","e26f89da8dcc451385037f3bd664f380","223c9f46e1ac426e8a10ff68690b73bd","cdbfe8cc0d564e939b277e1e46140640","afeae54ba892409eb532e1b30d81759c","18e403c2d3fd4642b1cbfe17bdd9963b","72a4f9f5d05e42c5a1cebdb90473958d","52e48b7ef9a147c4a660e56aad5308a8","94f06f73bd7d4f11bbe3ad9c22910daf","fb8e6c6f3d1640b19b53c84ec0b02a20","c7cb7e3106824396a14a757020fc221e"]},"outputId":"a61ba4b9-c90f-4256-8408-ade9460a03ae"},"execution_count":7,"outputs":[{"output_type":"display_data","data":{"text/plain":["Map (num_proc=4):   0%|          | 0/8436 [00:00<?, ? examples/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a92e8bea062f40118242e0915f67558f"}},"metadata":{}}]},{"cell_type":"code","source":["from peft import get_peft_model, LoraConfig\n","\n","# assume `model` is your base TinyLlama loaded already\n","peft_config = LoraConfig(\n","    task_type=\"CAUSAL_LM\",\n","    r=8,\n","    lora_alpha=32,\n","    lora_dropout=0.05\n",")\n","model = get_peft_model(model, peft_config)\n","\n"],"metadata":{"id":"LNAJmI3YDTt8","executionInfo":{"status":"ok","timestamp":1745952091833,"user_tz":240,"elapsed":109,"user":{"displayName":"Carl Wheezer","userId":"01232549880719749531"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["import torch\n","from torch.nn.utils.rnn import pad_sequence\n","from transformers import TrainingArguments, Trainer\n","\n","# 1) Custom collator pads input_ids & labels\n","def collate_fn(batch):\n","    # turn lists into tensors\n","    input_tensors = [torch.tensor(e[\"input_ids\"], dtype=torch.long) for e in batch]\n","    label_tensors = [torch.tensor(e[\"labels\"],    dtype=torch.long) for e in batch]\n","\n","    # pad them\n","    input_ids = pad_sequence(input_tensors, batch_first=True, padding_value=tokenizer.pad_token_id)\n","    labels    = pad_sequence(label_tensors, batch_first=True, padding_value=-100)\n","    attention_mask = input_ids.ne(tokenizer.pad_token_id).long()\n","\n","    return {\n","        \"input_ids\":      input_ids,\n","        \"attention_mask\": attention_mask,\n","        \"labels\":         labels,\n","    }\n","\n","# 2) Training args\n","training_args = TrainingArguments(\n","    output_dir=\"/content/drive/MyDrive/tinyllama-lora\",\n","    per_device_train_batch_size=4,\n","    num_train_epochs=2,\n","    fp16=torch.cuda.is_available(),  # GPU → True, CPU → False\n","    report_to=\"none\",                # disable wandb\n",")\n","\n","# 3) Trainer\n","trainer = Trainer(\n","    model=model,\n","    args=training_args,\n","    train_dataset=tok_ds,   # your tokenized dataset\n","    data_collator=collate_fn\n",")\n","\n","# 4) Train & save\n","trainer.train()\n","model.save_pretrained(\"/content/drive/MyDrive/tinyllama-lora\")\n","print(\"Done. Now listing:\")\n","!ls /content/drive/MyDrive/tinyllama-lora\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":450},"id":"vh3TEsysIYGq","executionInfo":{"status":"ok","timestamp":1745953123540,"user_tz":240,"elapsed":1029973,"user":{"displayName":"Carl Wheezer","userId":"01232549880719749531"}},"outputId":"ab7c3618-117d-4ffd-e0f8-c94b97431caa"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stderr","text":["No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='4218' max='4218' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [4218/4218 17:04, Epoch 2/2]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>500</td>\n","      <td>2.492900</td>\n","    </tr>\n","    <tr>\n","      <td>1000</td>\n","      <td>2.070600</td>\n","    </tr>\n","    <tr>\n","      <td>1500</td>\n","      <td>2.018300</td>\n","    </tr>\n","    <tr>\n","      <td>2000</td>\n","      <td>2.018100</td>\n","    </tr>\n","    <tr>\n","      <td>2500</td>\n","      <td>1.973000</td>\n","    </tr>\n","    <tr>\n","      <td>3000</td>\n","      <td>1.940000</td>\n","    </tr>\n","    <tr>\n","      <td>3500</td>\n","      <td>1.942400</td>\n","    </tr>\n","    <tr>\n","      <td>4000</td>\n","      <td>1.942500</td>\n","    </tr>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Done. Now listing:\n","adapter_config.json\t   checkpoint-2000  checkpoint-4000  runs\n","adapter_model.safetensors  checkpoint-2500  checkpoint-4218\n","checkpoint-1000\t\t   checkpoint-3000  checkpoint-500\n","checkpoint-1500\t\t   checkpoint-3500  README.md\n"]}]},{"cell_type":"code","source":["from transformers import AutoTokenizer, AutoModelForCausalLM\n","from peft import PeftModel\n","import torch\n","# fine-tuned model.\n","base = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n","tokenizer = AutoTokenizer.from_pretrained(base)\n","base_model = AutoModelForCausalLM.from_pretrained(\n","    base,\n","    torch_dtype=torch.float16,\n","    device_map=\"auto\"\n",")\n","model = PeftModel.from_pretrained(base_model, \"/content/drive/MyDrive/tinyllama-lora\")\n","\n","def respond(scenario: str, max_new_tokens=64):\n","    prompt = f\"{scenario}\\n\\n### Response:\\n\"\n","    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n","\n","    out = model.generate(\n","        **inputs,\n","        max_new_tokens=max_new_tokens,\n","        do_sample=True,\n","        temperature=0.7,\n","        top_p=0.9,\n","        repetition_penalty=1.2,    # discourage repeats\n","        no_repeat_ngram_size=3,    # ban 3-gram repetition\n","        eos_token_id=tokenizer.eos_token_id,\n","    )\n","\n","    text = tokenizer.decode(out[0], skip_special_tokens=True)\n","    # split off everything up to your marker\n","    return text.split(\"### Response:\")[-1].strip()\n","\n","# prompts\n","print(respond(\"Player interacts with a glowing rune\"))\n","print(respond(\"Player interacts with a sleeping dragon\"))\n","print(respond(\"Player interacts with a glowing dragon\"))\n","print(respond(\"Player interacts with a mischevious gnome\"))\n","print(respond(\"Player interacts with a rain-soaked map\"))\n","print(respond(\"Playafndsnfiodsfp\"))\n","print(respond(\"Player interacts with weird, fat man\"))\n","print(respond(\"Player interacts with Mr. Saturn\"))\n","print(respond(\"Player interacts with broom\"))\n","print(respond(\"Player interacts with ancient vending machine\"))\n","\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wv0HS5mjsZO0","executionInfo":{"status":"ok","timestamp":1745956017386,"user_tz":240,"elapsed":22285,"user":{"displayName":"Carl Wheezer","userId":"01232549880719749531"}},"outputId":"f2adf7cc-187e-4230-cbd1-804b8f8ee589"},"execution_count":14,"outputs":[{"output_type":"stream","name":"stdout","text":["The rune seems to pulse and shift, emitting an ethereal light. It speaks in tongues you can barely hear—a faint whisper of ancient magic. You don't understand it all, but the feeling is unmistakable—like you’ve been here before...\n","I’ll wake up when it feels like it. If you want to keep an eye on me, you can try. Otherwise... don’t. Not today. And not ever again unless the dragon tells you what time it is. Which it may or may not do. *snort* But\n","I’m not sure what you want. But if you don’t ask, I may never answer. And that would be unfortunate for all. Or so it seems. *snort* *shakes head* *dragon flaps wings and disappears into the night sky.*\n","You’re too curious. What are you looking for? A hidden treasure or something more substantial? Either way, I won’t tell if you don’t ask me first! But watch your step, there might be spikes in the ground... *laughter* *cough* *sn\n","The storm has washed away the roads, leaving only a trail of mud and memory. You trace it back to your grandfather’s house on the hill—and realize you don't know where he is anymore. He might have gone in search of the old way. But what if he never returns?\n","tatywna I am not afraid of the future. It scares me instead. And my cat.\n","I’m not like the rest. They say I walk in circles, but that’s just my way of saying ‘I don’t care about what you think.’ Or maybe I do? Who knows? Anyway, can we get this thing out of here already? It’s driving me insane!\n","I’ve been waiting for you to ask me a question. You know the one about time travel? Or maybe the one where I tell you to go home? Either way, it’s not my fault if you get lost in my cosmic labyrinth. Just don’t blame me when something\n","Cleaning doesn’t always feel clean. But it gets the job done. And maybe some dust on your shirt. Or a little luck in the form of mice or spiders. Either way, you’ll be sweeping cleaner than ever before!\n","It spits out a coin, then shakes its head. 'Not today.'\n"]}]},{"cell_type":"code","source":["# Peek at a batch after collator to confirm shapes\n","batch = data_collator([tok_ds[i] for i in range(4)])\n","print({k: v.shape for k,v in batch.items()})\n","# Expected: all tensors (input_ids, attention_mask, labels) have shape [4, L] for some L\n"],"metadata":{"id":"pMTig13vJpGZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!ls /content/drive\n","!ls /content/drive/MyDrive"],"metadata":{"id":"8dOevKU0t8FB","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1745949841910,"user_tz":240,"elapsed":337,"user":{"displayName":"Carl Wheezer","userId":"01232549880719749531"}},"outputId":"1d8ceb79-43ac-4725-fe1b-4e8d8097dd77"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["ls: cannot access '/content/drive': No such file or directory\n","ls: cannot access '/content/drive/MyDrive': No such file or directory\n"]}]},{"cell_type":"code","source":["!ls /content/drive/MyDrive/tinyllama-lora\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"q5il7jSntJmE","executionInfo":{"status":"ok","timestamp":1745949835108,"user_tz":240,"elapsed":110,"user":{"displayName":"Carl Wheezer","userId":"01232549880719749531"}},"outputId":"26e41d47-f03f-4dac-d774-9917b44e74a4"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["ls: cannot access '/content/drive/MyDrive/tinyllama-lora': No such file or directory\n"]}]},{"cell_type":"code","source":["# sanity-check of base TinyLlama\n","from transformers import AutoTokenizer, AutoModelForCausalLM\n","import torch\n","\n","model_name = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n","tokenizer  = AutoTokenizer.from_pretrained(model_name)\n","\n","if torch.cuda.is_available():\n","    model = AutoModelForCausalLM.from_pretrained(\n","        model_name,\n","        load_in_8bit=True,\n","        device_map=\"auto\"\n","    )\n","else:  # CPU fallback (slow)\n","    model = AutoModelForCausalLM.from_pretrained(\n","        model_name,\n","        device_map={\"\": \"cpu\"},\n","        torch_dtype=torch.float32\n","    )\n","\n","def base_respond(prompt: str, max_new_tokens=64):\n","    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n","    out    = model.generate(\n","        **inputs,\n","        max_new_tokens=max_new_tokens,\n","        do_sample=True,\n","        temperature=0.7,\n","        top_p=0.9,\n","        repetition_penalty=1.2,\n","        no_repeat_ngram_size=3,\n","        eos_token_id=tokenizer.eos_token_id\n","    )\n","    return tokenizer.decode(out[0], skip_special_tokens=True)\n","\n","# prompts\n","print(respond(\"Player interacts with a glowing rune\"))\n","print(respond(\"Player interacts with a sleeping dragon\"))\n","print(respond(\"Player interacts with a glowing dragon\"))\n","print(respond(\"Player interacts with a mischevious gnome\"))\n","print(respond(\"Player interacts with a rain-soaked map\"))\n","print(respond(\"Playafndsnfiodsfp\"))\n","print(respond(\"Player interacts with weird, fat man\"))\n","print(respond(\"Player interacts with Mr. Saturn\"))\n","print(respond(\"Player interacts with broom\"))\n","print(respond(\"Player interacts with ancient vending machine\"))\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WGBpEYtmFutM","executionInfo":{"status":"ok","timestamp":1745956150935,"user_tz":240,"elapsed":50710,"user":{"displayName":"Carl Wheezer","userId":"01232549880719749531"}},"outputId":"8a0c7333-74d8-4ec7-ff55-857302874e93"},"execution_count":16,"outputs":[{"output_type":"stream","name":"stderr","text":["The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n"]},{"output_type":"stream","name":"stdout","text":["The player is greeted by the glow of a rune that surrounds them. The runes light up, revealing the following message: \"Welcome to Luminosity's mystical world! Let your imagination take flight as you explore our vast and enchanting universe.\"\n","Player: (whispers) I've never slept so soundly in my life.\n","\"Wow, that sounds incredible! I'm so excited to see what this game can do. But before we dive in, could you tell me more about the different types of players? Are there any who prefer single-player experiences or those who thrive on competition?\"\n","Gnomes are known for their love of stealing and causing chaos wherever they go. However, this story is not about a mischievous gnome but instead it's about the dangers that come from taking advantage of people who need help or assistance. The interaction between the player character and the g\n","\"It's raining, and the ground is slick. Be careful out there!\" The player looks at the environment around them, noticing that it has taken on a more moist texture due to the rain falling heavily from above. They step carefully as they approach the object in question, feeling the wet\n","```json\n","{\n","  \"message\": \"The given data was invalid.\",\n","  \"_links\": {\n","    \"self\": { \"href\": \"/api/v1.0/users\" }\n","  },\n","  \"@context\": { \"@type\": \"http://schema.org/\" }\n","}\n","I'm glad you enjoyed the new story! Here's a revised version that includes more dialogue and explores the player character's emotional journey. \n","\n","Title: The Unknown\n","\n","Characters:\n","- John, a middle-aged accountant\n","- Sarah, his wife\n","[Saturn laughs] \"Ah, yes, the great interdimensional traveler.\" [He gestures to a nearby orb] \"This is what I have been searching for all these years - an alien race that can communicate through dreams and visions.\" [To the player character] \"The dream\n","Broom is a magical weapon that can be used to shape-shift into different forms. It was created by the ancient witches and has been passed down through generations, giving it incredible power and wisdom. The player should use their knowledge of magic and acquire this powerful artifact in order to defeat the\n","The ancient vender's player has been successfully activated and is now able to purchase items from the machine. The player can either choose an item by clicking on it, or they may enter a price for the desired item and click \"Buy\" to add it to their inventory. If the player would like\n"]}]},{"cell_type":"code","source":["import random\n","\n","IN  = \"/content/drive/My Drive/WizardDatasetFormatted.jsonl\"\n","TRN = \"/content/drive/My Drive/wizard_train.jsonl\"\n","EVAL= \"/content/drive/My Drive/wizard_eval.jsonl\"\n","\n","with open(IN, \"r\", encoding=\"utf-8\") as f:\n","    lines = [l for l in f if l.strip()]\n","\n","random.seed(42)\n","random.shuffle(lines)\n","n_eval = int(0.1 * len(lines))\n","\n","with open(TRN, \"w\", encoding=\"utf-8\") as f:\n","    f.writelines(lines[n_eval:])\n","with open(EVAL, \"w\", encoding=\"utf-8\") as f:\n","    f.writelines(lines[:n_eval])\n","\n","print(f\"Train: {len(lines)-n_eval} lines, Eval: {n_eval} lines\")\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FsaCC9R-WjD5","executionInfo":{"status":"ok","timestamp":1745981381772,"user_tz":240,"elapsed":106,"user":{"displayName":"Carl Wheezer","userId":"01232549880719749531"}},"outputId":"dc426d8e-aa88-4fb4-c1a7-21c2ba423d4e"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["Train: 7593 lines, Eval: 843 lines\n"]}]},{"cell_type":"code","source":["from datasets import load_dataset\n","\n","ds = load_dataset(\n","    \"json\",\n","    data_files={\"eval\": \"/content/drive/My Drive/wizard_eval.jsonl\"},\n","    split=\"eval\"\n",")\n","\n","def pretokenize(ex):\n","    tokens = tokenizer(\n","        ex[\"prompt\"] + ex[\"completion\"],\n","        truncation=True,\n","        max_length=512\n","    )[\"input_ids\"]\n","    return {\"input_ids\": tokens, \"labels\": tokens.copy()}\n","\n","tok_ds = ds.map(pretokenize, remove_columns=ds.column_names, num_proc=4)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":81,"referenced_widgets":["87687cffba6743cabdd1b0820efb3cd1","ab9133cf35c84eab93c92b4d98417588","bcae3dd2a45040d193545bef33275c3f","f13bb79e22f842908103785ac226e92f","075e84bbd44e4655b637977aaa0217c8","8888e63277154924a2e99d70b8eb2008","32b50e9225f94c3cbe5b8cf996ebf1e5","0c52485a74c4466888ec5f0cc6617bce","2da4c3f6dd244de591e831ab4e33c5af","1bc97e08c46342e181f4309c7501c5bc","7fdd1615f7534d8d8f29cdd290c01f7a","6e741ed233eb481ebaaf85d20347734e","83bc4884fdcb42eea6bab4fcefccaf0c","3ecdc59d10724254922dfdf750cb2db3","05e41235d32e4d4f89545bbfbe8cd805","0cc6371b502e4e0794cd7ec000d9c986","3a5b42a9ae024ed5a8c678cc7f92ea38","7420bd142e564498959a47a01804fb99","60ccea7ae58d47c693b82f70dfafd50d","2c455a76b78f4a74b021d028dc75fcdf","64b3e180d5b243ddba1f375a0d02b983","0a5ead617fa34b1d9d2ff357638e3223"]},"id":"TpNEbSJ6WkhR","executionInfo":{"status":"ok","timestamp":1745981386201,"user_tz":240,"elapsed":2552,"user":{"displayName":"Carl Wheezer","userId":"01232549880719749531"}},"outputId":"0edd177d-a3ad-42b3-ba7a-ccad1dd45808"},"execution_count":7,"outputs":[{"output_type":"display_data","data":{"text/plain":["Generating eval split: 0 examples [00:00, ? examples/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"87687cffba6743cabdd1b0820efb3cd1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Map (num_proc=4):   0%|          | 0/843 [00:00<?, ? examples/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6e741ed233eb481ebaaf85d20347734e"}},"metadata":{}}]},{"cell_type":"code","source":["print(ft.base_model.model)\n","print(ft.peft_config)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sAOqoJh7kRSI","executionInfo":{"status":"ok","timestamp":1745980868804,"user_tz":240,"elapsed":20,"user":{"displayName":"Carl Wheezer","userId":"01232549880719749531"}},"outputId":"42f0d91b-1f57-4375-db3f-b2745a07bca5"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["LlamaForCausalLM(\n","  (model): LlamaModel(\n","    (embed_tokens): Embedding(32000, 2048)\n","    (layers): ModuleList(\n","      (0-21): 22 x LlamaDecoderLayer(\n","        (self_attn): LlamaAttention(\n","          (q_proj): lora.Linear(\n","            (base_layer): Linear(in_features=2048, out_features=2048, bias=False)\n","            (lora_dropout): ModuleDict(\n","              (default): Dropout(p=0.05, inplace=False)\n","            )\n","            (lora_A): ModuleDict(\n","              (default): Linear(in_features=2048, out_features=8, bias=False)\n","            )\n","            (lora_B): ModuleDict(\n","              (default): Linear(in_features=8, out_features=2048, bias=False)\n","            )\n","            (lora_embedding_A): ParameterDict()\n","            (lora_embedding_B): ParameterDict()\n","            (lora_magnitude_vector): ModuleDict()\n","          )\n","          (k_proj): Linear(in_features=2048, out_features=256, bias=False)\n","          (v_proj): lora.Linear(\n","            (base_layer): Linear(in_features=2048, out_features=256, bias=False)\n","            (lora_dropout): ModuleDict(\n","              (default): Dropout(p=0.05, inplace=False)\n","            )\n","            (lora_A): ModuleDict(\n","              (default): Linear(in_features=2048, out_features=8, bias=False)\n","            )\n","            (lora_B): ModuleDict(\n","              (default): Linear(in_features=8, out_features=256, bias=False)\n","            )\n","            (lora_embedding_A): ParameterDict()\n","            (lora_embedding_B): ParameterDict()\n","            (lora_magnitude_vector): ModuleDict()\n","          )\n","          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n","        )\n","        (mlp): LlamaMLP(\n","          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)\n","          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)\n","          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)\n","          (act_fn): SiLU()\n","        )\n","        (input_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n","        (post_attention_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n","      )\n","    )\n","    (norm): LlamaRMSNorm((2048,), eps=1e-05)\n","    (rotary_emb): LlamaRotaryEmbedding()\n","  )\n","  (lm_head): Linear(in_features=2048, out_features=32000, bias=False)\n",")\n","{'default': LoraConfig(task_type='CAUSAL_LM', peft_type=<PeftType.LORA: 'LORA'>, auto_mapping=None, base_model_name_or_path='TinyLlama/TinyLlama-1.1B-Chat-v1.0', revision=None, inference_mode=True, r=8, target_modules={'q_proj', 'v_proj'}, exclude_modules=None, lora_alpha=32, lora_dropout=0.05, fan_in_fan_out=False, bias='none', use_rslora=False, modules_to_save=None, init_lora_weights=True, layers_to_transform=None, layers_pattern=None, rank_pattern={}, alpha_pattern={}, megatron_config=None, megatron_core='megatron.core', trainable_token_indices=None, loftq_config={}, eva_config=None, corda_config=None, use_dora=False, layer_replication=None, runtime_config=LoraRuntimeConfig(ephemeral_gpu_offload=False), lora_bias=False)}\n"]}]},{"cell_type":"code","source":["import math, torch\n","from datasets import load_dataset\n","from torch.utils.data import DataLoader\n","from torch.nn.utils.rnn import pad_sequence\n","from transformers import AutoTokenizer, AutoModelForCausalLM\n","from peft import PeftModel\n","\n","device = torch.device(\"cpu\")\n","\n","# ─── A) Load held‐out split ─────────────────────────────────────────────────\n","ds = load_dataset(\n","    \"json\",\n","    data_files={\"eval\": \"/content/drive/My Drive/wizard_eval.jsonl\"},\n","    split=\"eval\"\n",")\n","\n","# ─── B) Tokenizer + new pretok ─────────────────────────────────────────────\n","tokenizer = AutoTokenizer.from_pretrained(\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\")\n","\n","def pretok(ex):\n","    # tokenize prompt and completion separately\n","    p = tokenizer(ex[\"prompt\"],      truncation=True, max_length=512)\n","    c = tokenizer(ex[\"completion\"],  truncation=True, max_length=128)\n","    # full input IDs = prompt IDs + completion IDs\n","    input_ids = p[\"input_ids\"] + c[\"input_ids\"]\n","    # labels: ignore prompt, only predict completion\n","    labels    = [-100] * len(p[\"input_ids\"]) + c[\"input_ids\"]\n","    return {\"input_ids\": input_ids, \"labels\": labels}\n","\n","# Re‐map to create tok_ds with the new labels scheme\n","tok_ds = ds.map(\n","    pretok,\n","    remove_columns=ds.column_names,\n","    num_proc=4\n",")\n","\n","# ─── C) Collator + DataLoader ──────────────────────────────────────────────\n","def collate(batch):\n","    inp = [torch.tensor(e[\"input_ids\"]) for e in batch]\n","    lbl = [t.clone()                   for t in inp]\n","    inp = pad_sequence(inp, batch_first=True,\n","                       padding_value=tokenizer.pad_token_id)\n","    lbl = pad_sequence(lbl, batch_first=True,\n","                       padding_value=-100)\n","    return {\"input_ids\": inp, \"labels\": lbl}\n","\n","loader = DataLoader(tok_ds, batch_size=16, collate_fn=collate)\n","\n","# ─── D) PPL helper ─────────────────────────────────────────────────────────\n","def compute_ppl(model):\n","    total_loss, total_tokens = 0.0, 0\n","    model.eval()\n","    with torch.no_grad():\n","        for batch in loader:\n","            batch = {k: v.to(model.device) for k,v in batch.items()}\n","            out = model(**batch)\n","            L = out.loss.item()\n","            n = (batch[\"labels\"] != -100).sum().item()\n","            total_loss  += L * n\n","            total_tokens += n\n","    return math.exp(total_loss / total_tokens)\n","\n","# ─── E) Base vs. Fine-tuned PPL ─────────────────────────────────────────────\n","# Load a fresh base model for PPL\n","base1 = AutoModelForCausalLM.from_pretrained(\n","    \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\",\n","    device_map={\"\": \"cpu\"}, torch_dtype=torch.float32\n",").to(device)\n","\n","# Load a second base and attach the LoRA adapter\n","base2 = AutoModelForCausalLM.from_pretrained(\n","    \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\",\n","    device_map={\"\": \"cpu\"}, torch_dtype=torch.float32\n",").to(device)\n","ft = PeftModel.from_pretrained(\n","    base2,\n","    \"/content/drive/My Drive/tinyllama-lora\",\n","    device_map={\"\": \"cpu\"},\n","    torch_dtype=torch.float32,\n","    local_files_only=True\n",").to(device)\n","\n","# Compute and print\n","ppl_base = compute_ppl(base1)\n","ppl_ft   = compute_ppl(ft)\n","print(f\"Base model PPL:      {ppl_base:.2f}\")\n","print(f\"Fine-tuned model PPL: {ppl_ft:.2f}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xM3UcgQ4xrsB","executionInfo":{"status":"ok","timestamp":1745992524942,"user_tz":240,"elapsed":2618569,"user":{"displayName":"Carl Wheezer","userId":"01232549880719749531"}},"outputId":"59b9aafa-a9da-44db-ba3e-16fc68666378"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["Base model PPL:      154.23\n","Fine-tuned model PPL: 36.25\n"]}]}]}